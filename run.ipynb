{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "551p4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_bawXg94jf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "# start = time.time()\n",
        "\n",
        "# time.sleep(10)  # or do something more productive\n",
        "\n",
        "# done = time.time()\n",
        "# elapsed = done - start\n",
        "# print(elapsed)\n",
        "# time.monotonic()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih_999N24r8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Kmeans(object):\n",
        "    '''\n",
        "    In-house implementation of k-means via Lloyd-Max iterations\n",
        "    This is a research prototype and is not necessarily well-optimized\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                    k,\n",
        "                    termination='fixed',\n",
        "                    iters=10,\n",
        "                    tol=10**-3):\n",
        "        '''\n",
        "        Constructor\n",
        "        INPUT:\n",
        "        k - # of centroids/clusters\n",
        "        iters - # of iterations to run\n",
        "        termination - {'fixed', 'loss', 'centers'}\n",
        "            if 'fixed' - runs for fixed # of iterations\n",
        "            if 'loss' - runs until loss converges\n",
        "            if 'centers' -runs until centers converge\n",
        "        tol - numeric tolerance to determine convergence\n",
        "        '''\n",
        "        # set parameters\n",
        "        self.k = k\n",
        "        self.iters = iters\n",
        "        self.tol = tol\n",
        "        self.termination = termination\n",
        "        # initialize placeholder values\n",
        "        self._init_placeholders()\n",
        "\n",
        "        self.retrain = 0\n",
        "\n",
        "    def run(self, X):\n",
        "        '''\n",
        "        Run clustering algorithm\n",
        "        INPUT:\n",
        "        X - numpy matrix, n-by-d, each row is a data point\n",
        "        OUTPUT: (3-tuple)\n",
        "        centroids - k-by-d matrix of centroids\n",
        "        assignments - Vector of length n, with datapoint to center assignments\n",
        "        loss - The loss of the final partition\n",
        "        '''\n",
        "        self._set_data(X)\n",
        "        self._lloyd_iterations()\n",
        "        return self.centroids, self.assignments, self.loss\n",
        "\n",
        "    def delete(self, del_idx):\n",
        "        '''\n",
        "        Delete point associated with key del_idx\n",
        "        NOTE: del_idx must be int in {0,n-1}\n",
        "            After deleting any key other than n-1,\n",
        "            the (n-1)-th datapoint's key is automatically\n",
        "            swapped with del_idx to\n",
        "        '''\n",
        "        self.data = np.delete(self.data, del_idx, 0)\n",
        "        self.n = self.n-1\n",
        "        self._init_placeholders()\n",
        "        return self.run(self.data)\n",
        "\n",
        "    def _init_placeholders(self):\n",
        "        self.loss = np.Infinity\n",
        "        self.empty_clusters = []\n",
        "        self.kpp_inits = set()\n",
        "        self.centroids = None\n",
        "        self.assignments = None\n",
        "        self.model = None\n",
        "\n",
        "    def _set_data(self, X):\n",
        "        self.data = X\n",
        "        self.n, self.d = X.shape\n",
        "\n",
        "    def _lloyd_iterations(self):\n",
        "        self._init_centroids()\n",
        "        for _ in range(self.iters):\n",
        "            loss_prev = self.loss\n",
        "            centers_prev = self.model\n",
        "            self._assign_clusters()\n",
        "            self._assign_centroids()\n",
        "            prev = loss_prev if self.termination == 'loss' else centers_prev\n",
        "            if self._check_termination(prev):\n",
        "                break\n",
        "            \n",
        "    def _check_termination(self, prev):\n",
        "        if self.termination == 'loss':\n",
        "            return (1 - self.loss/prev) < self.tol\n",
        "        elif self.termination == 'center':\n",
        "            return np.linalg.norm(self.centroids - prev) < self.tol\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def _init_centroids(self):\n",
        "        '''\n",
        "        Kmeans++ initialization\n",
        "        Returns vector of initial centroids\n",
        "        '''\n",
        "        first_idx = np.random.choice(self.n)\n",
        "        self.centroids = self.data[first_idx,:]\n",
        "        for kk in range(1,self.k):\n",
        "            P = self._get_selection_prob()\n",
        "            nxt_idx = np.random.choice(self.n,p=P)\n",
        "            self.kpp_inits.add(nxt_idx)\n",
        "            self.centroids = np.vstack([self.centroids,self.data[nxt_idx,:]])\n",
        "\n",
        "    def _get_selection_prob(self):\n",
        "        '''\n",
        "        Outputs vector of selection probabilites\n",
        "        Equal to Distance^2 to nearest centroid\n",
        "        '''\n",
        "        #handle edge case in centroids shape by unsqueezing\n",
        "        if len(self.centroids.shape) == 1:\n",
        "            self.centroids = np.expand_dims(self.centroids, axis=0)\n",
        "\n",
        "        #probability is square distance to closest centroid\n",
        "        D = np.zeros([self.n])\n",
        "        for i in range(self.n):\n",
        "            d = np.linalg.norm(self.data[i,:] - self.centroids, axis=1)\n",
        "            D[i] = np.min(d)\n",
        "        P = [dist**2 for dist in D]\n",
        "        P = P / sum(P)\n",
        "        return P  \n",
        "\n",
        "    def _assign_centroids(self):\n",
        "        '''\n",
        "        Computes centroids in Lloyd iterations\n",
        "        '''\n",
        "        self.centroids = np.zeros([self.k,self.d])\n",
        "        c = np.zeros([self.k])\n",
        "        for i in range(self.n):\n",
        "            a = self.assignments[i]\n",
        "            c[a] += 1\n",
        "            self.centroids[a,:] += self.data[i,:]\n",
        "            \n",
        "        for j in range(self.k):\n",
        "            self.centroids[j,:] = self.centroids[j,:] / c[j]\n",
        "\n",
        "        for j in self.empty_clusters: \n",
        "            self._reinit_cluster(j)\n",
        "        self.empty_clusters = []\n",
        "        \n",
        "    def _assign_clusters(self):\n",
        "        '''\n",
        "        Computes clusters in Lloyd iterations\n",
        "        '''\n",
        "        assert (self.k, self.d) == self.centroids.shape, \"Centers wrong shape\"\n",
        "        self.assignments = np.zeros([self.n]).astype(int)\n",
        "        self.loss = 0\n",
        "        for i in range(self.n):\n",
        "            d = np.linalg.norm(self.data[i,:] - self.centroids, axis=1)\n",
        "            d1 = np.linalg.norm(self.data[i,:] - self.centroids, axis=1,ord=1)\n",
        "            self.assignments[i] = int(np.argmin(d))\n",
        "            self.loss += np.min(d)**2\n",
        "        self.loss = self.loss / self.n\n",
        "        self.empty_clusters = self._check_4_empty_clusters()\n",
        "\n",
        "    def _check_4_empty_clusters(self):\n",
        "        empty_clusters = []\n",
        "        for kappa in range(self.k):\n",
        "            if len(np.where(self.assignments == kappa)[0]) == 0:\n",
        "                empty_clusters.append(kappa)\n",
        "        return empty_clusters\n",
        "\n",
        "    def _reinit_cluster(self, j):\n",
        "        '''\n",
        "        Gets a failed centroid with idx j (empty cluster)\n",
        "        Should replace with new k++ init centroid\n",
        "        in:\n",
        "            j is idx for centroid, 0 <= j <= n\n",
        "        out:\n",
        "            j_prime is idx for next centroid\n",
        "        side-effects:\n",
        "            centroids are update to reflect j -> j'\n",
        "        '''\n",
        "        P = self._get_selection_prob()\n",
        "        j_prime = np.random.choice(self.n,p=P)\n",
        "        self.kpp_inits.add(j_prime)\n",
        "        self.centroids[j,:] = self.data[j_prime,:]\n",
        "        return j_prime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umjeFwuJ4vtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QKmeans(Kmeans):\n",
        "    def __init__(self,\n",
        "                    k,\n",
        "                    eps,\n",
        "                    termination='fixed',\n",
        "                    iters=10, \n",
        "                    gamma=0.2,\n",
        "                    tol=10**-3):\n",
        "        '''\n",
        "        Constructor for quantized k-means solved via Lloyd iterations\n",
        "        k - # of centroids/clusters\n",
        "        eps - epsilon parameter in quantizing epsilon net\n",
        "        termination - {'fixed', 'centers'}\n",
        "        iters - # of iterations to run\n",
        "        gamma - momentum correct parameter for class imbalance\n",
        "        tol - numerical convergence tolerance \n",
        "        '''\n",
        "        assert termination != 'loss','Termination should be fixed or centers' \n",
        "        Kmeans.__init__(self, k, termination=termination, iters=iters, tol=tol)\n",
        "        self.eps = eps\n",
        "        self.gamma = gamma\n",
        "        self.retrain = 0\n",
        "        \n",
        "    def run(self, X):\n",
        "        '''\n",
        "        X - numpy matrix, n-by-d, each row is a data point\n",
        "        OUTPUT: (3-tuple)\n",
        "        centroids - k-by-d matrix of centroids\n",
        "        assignments - Vector of length n, with datapoint to center assignments\n",
        "        loss - The loss of the final partition\n",
        "        '''\n",
        "        self._set_data(X)\n",
        "        self._init_placeholders_q()\n",
        "        self._init_metadata()\n",
        "        self._quant_lloyd_iterations()\n",
        "        return self.model, self.model_assignment, self.minloss, self.retrain\n",
        "\n",
        "    def delete(self, del_idx):\n",
        "        if not self._certify_invariance(del_idx):\n",
        "            self._init_placeholders_q()\n",
        "            self.retrain = self.retrain + 1\n",
        "            return super(QKmeans, self).delete(del_idx)\n",
        "        else:\n",
        "            return self.model, self.model_assignment, self.minloss, self.retrain\n",
        "\n",
        "    def metadata(self):\n",
        "        '''\n",
        "        Returns the metadata\n",
        "        '''\n",
        "        return self.c_record, self.phase_record\n",
        "\n",
        "    def _init_metadata(self):\n",
        "        self.analog_c_meta = np.zeros( [self.iters+1, self.k, self.d])\n",
        "        self.q_c_meta = np.zeros( [self.iters+1, self.k, self.d])\n",
        "        self.phase_record = np.zeros([self.iters, self.d])\n",
        "        self.clustersizes_record = np.zeros([self.iters, self.k])\n",
        "        self.early_term = -1\n",
        "        self.momentum = self.gamma * self.n / self.k\n",
        "\n",
        "    def _init_placeholders_q(self):\n",
        "        self.minloss = np.Infinity\n",
        "        self.momentum = None\n",
        "        self.model_assignment = None\n",
        "        self.model = None\n",
        "        self.analog_c_record = None\n",
        "        self.c_record = None\n",
        "        self.phase_record = None\n",
        "        self.clustersizes_record = None\n",
        "\n",
        "    def _certify_invariance(self, del_idx):\n",
        "        '''\n",
        "        Computes a certficate of invariance under deletion for del_idx\n",
        "        INPUT:\n",
        "        del_idx - int index of row of data matrix to delete\n",
        "        OUTPUT: \n",
        "        Succesful - Bool flag if deletion succesful\n",
        "            Updates metadata automatically on success\n",
        "        '''\n",
        "        pt2del = self.data[del_idx,:]\n",
        "        if del_idx in self.kpp_inits:\n",
        "            # print('retrained')\n",
        "            return False\n",
        "        \n",
        "        for i in range(self.iters):\n",
        "            if i >= self.early_term and self.early_term >= 0:\n",
        "                break\n",
        "                \n",
        "            analog_centroids  = self.analog_c_meta[i+1,:,:]\n",
        "            phase = self.phase_record[i,:]\n",
        "            clustersizes = self.clustersizes_record[i,:]\n",
        "            d = np.linalg.norm(pt2del - analog_centroids, axis=1)\n",
        "            assignment_idx = int(np.argmin(d))\n",
        "            centroid = analog_centroids[assignment_idx,:]\n",
        "            centroid_prev = self.q_c_meta[i,assignment_idx,:]\n",
        "            clustersize = clustersizes[assignment_idx]\n",
        "            \n",
        "            if clustersize < self.momentum:\n",
        "                centroid = self._momentum_correction(\n",
        "                    centroid, centroid_prev, clustersize)\n",
        "                clustersize = self.momentum\n",
        "                \n",
        "            perturbed_centroid = centroid - pt2del/clustersize\n",
        "            quant = self._Q(centroid, self.eps, phase)\n",
        "            quant_perturbed = self._Q(perturbed_centroid, self.eps, phase)\n",
        "            \n",
        "            if not all(quant == quant_perturbed):\n",
        "                # print('retrained')\n",
        "                return False\n",
        "            \n",
        "            self.clustersizes_record[i-1,assignment_idx] -= 1\n",
        "            self.analog_c_meta[i-1,assignment_idx] = perturbed_centroid\n",
        "                \n",
        "        # self.data[del_idx,:] = np.zeros(self.d)\n",
        "        self.data = np.delete(self.data, del_idx, 0)\n",
        "        self.n -= 1\n",
        "        self.momentum = self.gamma * self.n / self.k\n",
        "        return True\n",
        "\n",
        "    def _quant_lloyd_iterations(self):\n",
        "        self._init_centroids()\n",
        "        self.analog_c_meta[0] = self.centroids\n",
        "        #no need to quantize initial centroids\n",
        "        self.q_c_meta[0] = self.centroids \n",
        "        self._assign_clusters()\n",
        "        self._phase_shift()\n",
        "        for i in range(self.iters):\n",
        "            prev = self.model\n",
        "            self._iterate(i)\n",
        "            if self.minloss <= self.loss:\n",
        "                self.early_term = i\n",
        "                break\n",
        "            elif self._check_termination(prev):\n",
        "                self.early_term = i\n",
        "                self._save_model()\n",
        "                break\n",
        "            else:\n",
        "                self._save_model()\n",
        "\n",
        "    def _save_model(self):\n",
        "        self.minloss = self.loss\n",
        "        self.model = self.centroids\n",
        "        self.model_assignment = self.assignments\n",
        "\n",
        "    def _iterate(self,i):\n",
        "        self._assign_centroids()\n",
        "        self._quantize(i)\n",
        "        self._assign_clusters()\n",
        "\n",
        "    def _quantize(self,i):\n",
        "        #record analog clusters\n",
        "        self.analog_c_meta[i+1,:,:] = self.centroids\n",
        "\n",
        "        #compute the clustersizes\n",
        "        clustersizes = {j : 0 for j in range(self.k)}\n",
        "        for j in range(self.n):\n",
        "            a = self.assignments[j]\n",
        "            clustersizes[a] += 1\n",
        "  \n",
        "        #record the clustersizes and apply momentum correction\n",
        "        for j in range(self.k):\n",
        "            self.clustersizes_record[i,j] =  clustersizes[j]\n",
        "            if (clustersizes[j]  < self.momentum):\n",
        "                self.centroids[j] = self._momentum_correction(\n",
        "                    self.centroids[j], self.q_c_meta[i,j], clustersizes[j])\n",
        "        #quantize centroids\n",
        "        self._phase_shift()\n",
        "        self.centroids = self._Q(self.centroids, self.eps, self.phase)\n",
        "\n",
        "        #record random phase and quantized centroids\n",
        "        self.phase_record[i,:] = self.phase\n",
        "        self.q_c_meta[i+1] = self.centroids\n",
        "\n",
        "    def _phase_shift(self):\n",
        "        self.phase = np.random.random([self.d])\n",
        "\n",
        "    def _momentum_correction(self,centroid_cur, centroid_prev, clustersize):\n",
        "        lag = (self.momentum-clustersize)/self.momentum\n",
        "        lagged_centroid = (clustersize/self.momentum)*centroid_cur\n",
        "        lagged_centroid += lag*centroid_prev\n",
        "        return lagged_centroid\n",
        "\n",
        "    def _Q(self, centroids, eps, phase):\n",
        "        centroids = centroids * 1/eps        \n",
        "        centroids = centroids + (phase-0.5)\n",
        "        centroids = np.round(centroids)\n",
        "        centroids = centroids - (phase-0.5)\n",
        "        centroids = centroids * eps\n",
        "        return centroids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcTF1cQW47ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCnode(Kmeans):\n",
        "    '''\n",
        "    A k-means subproblem for the divide-and-conquer tree\n",
        "    in DC-k-means algorithm\n",
        "    '''\n",
        "    def __init__(self, k, iters):\n",
        "        Kmeans.__init__(self, k, iters=iters)\n",
        "        self.children = []\n",
        "        self.parent = None\n",
        "        self.time = 0\n",
        "        self.loss = 0\n",
        "        self.node_data = set()\n",
        "        self.data_prop = set()\n",
        "\n",
        "    def _run_node(self, X):\n",
        "        self._set_node_data(X)\n",
        "        self._lloyd_iterations()\n",
        "\n",
        "    def _set_node_data(self, X):\n",
        "        self.data = X[list(self.node_data)]\n",
        "        self._set_data(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPd9NhJx49rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCKmeans():\n",
        "    def __init__(self, ks, widths, iters=10):\n",
        "        '''\n",
        "        Constructor for quantized k-means solved via Lloyd iterations\n",
        "        ks - list of k parameter for each layer of DC-tree\n",
        "        widths - list of width parameter (number of buckets) for each layer\n",
        "        iters - # of iterations to run \n",
        "            (at present, only supports fixed iteration termination)\n",
        "        '''\n",
        "        self.ks = ks\n",
        "        self.widths = widths\n",
        "        self.dc_tree = self._init_tree(ks,widths,iters)\n",
        "        self.data_partition_table = dict()\n",
        "        self.data = dict()\n",
        "        self.dels = set()\n",
        "        self.valid_ids = []\n",
        "        self.d = 0\n",
        "        self.n = 0\n",
        "        self.h = len(self.dc_tree)\n",
        "        for i in range(self.h):\n",
        "            self.data[i] = None\n",
        "            \n",
        "    def run(self, X, assignments=False):\n",
        "        '''\n",
        "        X - numpy matrix, n-by-d, each row is a data point\n",
        "        assignments (optional) - bool flag, computes assignments and loss\n",
        "            NOTE: Without assignments flag, this only returns the centroids\n",
        "        OUTPUT:\n",
        "        centroids - k-by-d matrix of centroids\n",
        "            IF assignments FLAG IS SET ALSO RETURNS:\n",
        "        assignments - Vector of length n, with datapoint to center assignments\n",
        "        loss - The loss of the final partition\n",
        "        '''\n",
        "        self._init_data(X)\n",
        "        self._partition_data(X)\n",
        "        self._run()\n",
        "        if assignments:\n",
        "            assignment_solver = Kmeans(self.ks[0])\n",
        "            assignment_solver._set_data(X)\n",
        "            assignment_solver.centroids = self.centroids\n",
        "            assignment_solver._assign_clusters()\n",
        "            self.assignments = assignment_solver.assignments\n",
        "            self.loss = assignment_solver.loss\n",
        "            return self.centroids, self.assignments, self.loss\n",
        "        return self.centroids\n",
        "    \n",
        "    def delete(self, del_idx):\n",
        "        idx = self.valid_ids[del_idx]\n",
        "        self.valid_ids[del_idx] = self.valid_ids.pop()\n",
        "        self.dels.add(idx)\n",
        "        node = self.dc_tree[-1][self.data_partition_table[idx]]\n",
        "        node.node_data.remove(idx)\n",
        "        l = self.h-1\n",
        "        self.n -= 1\n",
        "        while True:\n",
        "            node._run_node(self.data[l])\n",
        "            if node.parent == None:\n",
        "                self.centroids = node.centroids\n",
        "                break\n",
        "            data_prop = list(node.data_prop)\n",
        "            for c_id in range(len(node.centroids)):\n",
        "                idx = data_prop[c_id]\n",
        "                self.data[l][idx] = node.centroids[c_id]\n",
        "            node = node.parent\n",
        "            l -= 1\n",
        "\n",
        "    def _init_data(self,X):\n",
        "        self.n = len(X)\n",
        "        self.valid_ids = list(range(self.n))\n",
        "        self.d = len(X[0])\n",
        "        data_layer_size = self.n\n",
        "        for i in range(self.h-1,-1,-1):\n",
        "            self.data[i] = np.zeros((data_layer_size,self.d))\n",
        "            data_layer_size = self.ks[i]*self.widths[i] \n",
        "        \n",
        "    def _partition_data(self, X):\n",
        "        self.d = len(X[0])\n",
        "        num_leaves = len(self.dc_tree[-1])\n",
        "        for i in range(len(X)):\n",
        "            leaf_id = np.random.choice(num_leaves)\n",
        "            leaf = self.dc_tree[-1][leaf_id]\n",
        "            self.data_partition_table[i] = leaf_id\n",
        "            leaf.node_data.add(i)\n",
        "            self.data[self.h-1][i] = X[i]\n",
        "\n",
        "    def _run(self):\n",
        "        for l in range(self.h-1,-1,-1):\n",
        "            c = 0\n",
        "            for j in range(self.widths[l]):\n",
        "                subproblem = self.dc_tree[l][j]\n",
        "                subproblem._run_node(self.data[l])\n",
        "                if subproblem.parent == None:\n",
        "                    self.centroids = subproblem.centroids\n",
        "                else:\n",
        "                    for c_id in range(len(subproblem.centroids)):\n",
        "                        subproblem.data_prop.add(c)\n",
        "                        subproblem.parent.node_data.add(c)\n",
        "                        self.data[l-1][c] = subproblem.centroids[c_id] \n",
        "                        c += 1\n",
        "\n",
        "    def _init_tree(self, ks, widths, iters):\n",
        "        tree = [[DCnode(ks[0],iters)]] # root node\n",
        "        for i in range(1,len(widths)):\n",
        "            k = ks[i]\n",
        "            assert widths[i] % widths[i-1] == 0, \"Inconsistent widths in tree\"\n",
        "            merge_factor = int(widths[i] / widths[i-1])\n",
        "            level = []\n",
        "            for j in range(widths[i-1]):\n",
        "                parent = tree[i-1][j]\n",
        "                for _ in range(merge_factor):\n",
        "                    child = DCnode(k,iters=10)\n",
        "                    child.parent = parent\n",
        "                    parent.children.append(child)\n",
        "                    level.append(child)\n",
        "            tree.append(level)\n",
        "        return tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpHspugk5BFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gaussian\n",
        "\n",
        "n = 20000\n",
        "cov = np.ones((25,25)) * 0.5\n",
        "cov = np.diag(np.diag(cov))\n",
        "m1 = np.random.rand(25)\n",
        "m2 = np.random.rand(25)\n",
        "m3 = np.random.rand(25)\n",
        "m4 = np.random.rand(25)\n",
        "m5 = np.random.rand(25)\n",
        "\n",
        "cluster1 = np.random.multivariate_normal(m1, cov , n)\n",
        "cluster2 = np.random.multivariate_normal(m2, cov , n)\n",
        "cluster3 = np.random.multivariate_normal(m3, cov , n)\n",
        "cluster4 = np.random.multivariate_normal(m4, cov , n)\n",
        "cluster5 = np.random.multivariate_normal(m5, cov , n)\n",
        "\n",
        "data_4 = np.vstack((cluster1,cluster2,cluster3,cluster4,cluster5))\n",
        "np.random.shuffle(data_4)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3OvZOhtBJpx",
        "colab_type": "code",
        "outputId": "73548386-f895-483b-e6fc-e213c2f38726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "kmeans = Kmeans(5)\n",
        "centers, assignments, loss = kmeans.run(data_4.copy())\n",
        "print(f'Kmeans Clustering loss is {loss}')\n",
        "\n",
        "qKmeans = QKmeans(5, 0.01)\n",
        "centers, assignments, loss, retrain = qKmeans.run(data_4.copy())\n",
        "print(f'QKmeans Clustering loss is {loss}')\n",
        "print(f'QKmeans Clustering retrain is {retrain}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kmeans Clustering loss is 12.181517844089333\n",
            "QKmeans Clustering loss is 12.319001628626125\n",
            "QKmeans Clustering retrain is 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-LAZrWKCvH7",
        "colab_type": "code",
        "outputId": "6a79d33d-abe0-4e89-d520-9a3d12429da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "numDelete = 1000\n",
        "\n",
        "start = time.monotonic()\n",
        "\n",
        "for i in range(numDelete):\n",
        "  centers, assignments, loss, retrain = qKmeans.delete(random.randint(0,qKmeans.data.shape[0]))\n",
        "  if ((i+1) % 40) == 0:\n",
        "    done = time.monotonic()\n",
        "    elapsed = done - start\n",
        "    print(elapsed)\n",
        "    print(f'QKmeans deleting {i+1} data retrained for {retrain} times using amortized runtime {elapsed/(i+1)}')\n",
        "\n",
        "print(f'QKmeans retrained for {retrain} times')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "931.0353503999995\n",
            "QKmeans deleting 40 data retrained for 25 times using amortized runtime 23.27588375999999\n",
            "2017.3058332630007\n",
            "QKmeans deleting 80 data retrained for 54 times using amortized runtime 25.216322915787508\n",
            "2953.7890897689995\n",
            "QKmeans deleting 120 data retrained for 79 times using amortized runtime 24.614909081408328\n",
            "4114.878553905999\n",
            "QKmeans deleting 160 data retrained for 110 times using amortized runtime 25.717990961912495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcD9swbKdWmM",
        "colab_type": "code",
        "outputId": "5c628390-3f56-44e7-affd-334ca6bea736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "for w in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]:\n",
        "  ks = np.ones(2, dtype=int) * 5\n",
        "  widths = np.ones(2, dtype=int) * w #w\n",
        "  widths[0] = 1\n",
        "  start = time.monotonic()\n",
        "  dCKmeans = DCKmeans(ks, widths)\n",
        "  centers, assignments, loss = dCKmeans.run(data_4.copy(), assignments=True)\n",
        "  print(f'DCKmeans Clustering loss is {loss}')\n",
        "\n",
        "  numDelete = 10\n",
        "\n",
        "  start = time.monotonic()\n",
        "\n",
        "  for i in range(numDelete):\n",
        "    dCKmeans.delete(random.randint(0,dCKmeans.n))\n",
        "    if ((i+1) % 10) == 0:\n",
        "      done = time.monotonic()\n",
        "      elapsed = done - start\n",
        "      print(elapsed)\n",
        "      print(f'dCKmeans with width {w} deleting {i+1} data using amortized runtime {elapsed/(i+1)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DCKmeans Clustering loss is 12.249068831973453\n",
            "325.72394888400004\n",
            "dCKmeans with width 1 deleting 10 data using amortized runtime 32.572394888400005\n",
            "DCKmeans Clustering loss is 12.249073899617887\n",
            "163.83242015700034\n",
            "dCKmeans with width 2 deleting 10 data using amortized runtime 16.383242015700034\n",
            "DCKmeans Clustering loss is 12.24909649451141\n",
            "81.4493562130001\n",
            "dCKmeans with width 4 deleting 10 data using amortized runtime 8.14493562130001\n",
            "DCKmeans Clustering loss is 12.25427205634391\n",
            "40.870271353000135\n",
            "dCKmeans with width 8 deleting 10 data using amortized runtime 4.087027135300014\n",
            "DCKmeans Clustering loss is 12.259030645116287\n",
            "20.782674412999768\n",
            "dCKmeans with width 16 deleting 10 data using amortized runtime 2.078267441299977\n",
            "DCKmeans Clustering loss is 12.58692900709338\n",
            "10.543209956999817\n",
            "dCKmeans with width 32 deleting 10 data using amortized runtime 1.0543209956999817\n",
            "DCKmeans Clustering loss is 12.258048311831981\n",
            "6.255208212999605\n",
            "dCKmeans with width 64 deleting 10 data using amortized runtime 0.6255208212999606\n",
            "DCKmeans Clustering loss is 12.258102700501974\n",
            "4.65094357199996\n",
            "dCKmeans with width 128 deleting 10 data using amortized runtime 0.46509435719999603\n",
            "DCKmeans Clustering loss is 12.292030048361644\n",
            "5.242221196000173\n",
            "dCKmeans with width 256 deleting 10 data using amortized runtime 0.5242221196000173\n",
            "DCKmeans Clustering loss is 12.706177904290604\n",
            "9.244700993999686\n",
            "dCKmeans with width 512 deleting 10 data using amortized runtime 0.9244700993999686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzH8cP-JVKGS",
        "colab_type": "code",
        "outputId": "ddf307bb-e653-48bf-891a-8afebe9c5d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "start = time.monotonic()\n",
        "kmeans = Kmeans(5)\n",
        "centers, assignments, loss = kmeans.run(data_4.copy())\n",
        "done = time.monotonic()\n",
        "elapsed = done - start\n",
        "print(f'Kmeans Clustering loss is {loss}, time is {elapsed}')\n",
        "\n",
        "epsilonList = [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25, 0.125]\n",
        "\n",
        "for e in epsilonList:\n",
        "  start = time.monotonic()\n",
        "  qKmeans = QKmeans(5, e)\n",
        "  centers, assignments, loss, retrain = qKmeans.run(data_4.copy())\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'QKmeans Clustering with epsilon {e} loss is {loss} using amortized runtime {elapsed}')\n",
        "\n",
        "widthList = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "\n",
        "for w in widthList:\n",
        "  ks = np.ones(2, dtype=int) * 5\n",
        "  widths = np.ones(2, dtype=int) * w\n",
        "  widths[0] = 1\n",
        "  start = time.monotonic()\n",
        "  dCKmeans = DCKmeans(ks, widths)\n",
        "  centers, assignments, loss = dCKmeans.run(data_4.copy(), assignments = True)\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'DCKmeans Clustering with width {w} loss is {loss} using amortized runtime {elapsed}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DCKmeans Clustering with width 1 loss is 12.208403869025302 using amortized runtime 38.32397420599955\n",
            "DCKmeans Clustering with width 2 loss is 12.202683067683678 using amortized runtime 38.62973167600012\n",
            "DCKmeans Clustering with width 4 loss is 12.240553346004154 using amortized runtime 38.77764523499991\n",
            "DCKmeans Clustering with width 8 loss is 12.20284884112078 using amortized runtime 38.77677151500029\n",
            "DCKmeans Clustering with width 16 loss is 12.430235578205632 using amortized runtime 39.0945337309995\n",
            "DCKmeans Clustering with width 32 loss is 12.219148401203746 using amortized runtime 38.679113144999974\n",
            "DCKmeans Clustering with width 64 loss is 12.2223916434676 using amortized runtime 38.86484926199955\n",
            "DCKmeans Clustering with width 128 loss is 12.237561897882237 using amortized runtime 39.55213801699938\n",
            "DCKmeans Clustering with width 256 loss is 12.5208976167683 using amortized runtime 39.91415930400035\n",
            "DCKmeans Clustering with width 512 loss is 12.57119339233539 using amortized runtime 40.607045977999405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ybnbCgsYBXE",
        "colab_type": "code",
        "outputId": "c56cbd30-f228-4d46-e0ed-cc955affde38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# mnist\n",
        "from keras.datasets import mnist\n",
        "import random\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "data_m0 = np.reshape(X_train,(60000,784))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXCwQVRyZNLF",
        "colab_type": "code",
        "outputId": "0d89a0c4-2ac7-42c8-90c2-1bde7534f9ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "kmeans = Kmeans(10)\n",
        "centers, assignments, loss = kmeans.run(data_4.copy())\n",
        "print(f'Kmeans Clustering loss is {loss}')\n",
        "\n",
        "qKmeans = QKmeans(10, 0.05)\n",
        "centers, assignments, loss, retrain = qKmeans.run(data_m0.copy())\n",
        "print(f'QKmeans Clustering loss is {loss}')\n",
        "print(f'QKmeans Clustering retrain is {retrain}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QKmeans Clustering loss is 2577545.7511303704\n",
            "QKmeans Clustering retrain is 0\n",
            "QKmeans retrained for 0 times\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxkM0fqVogV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "numDelete = 1000\n",
        "\n",
        "start = time.monotonic()\n",
        "\n",
        "for i in range(numDelete):\n",
        "  centers, assignments, loss, retrain = qKmeans.delete(random.randint(0,qKmeans.data.shape[0]))\n",
        "  if ((i+1) % 40) == 0:\n",
        "    done = time.monotonic()\n",
        "    elapsed = done - start\n",
        "    print(elapsed)\n",
        "    print(f'QKmeans deleting {i+1} data retrained for {retrain} times using amortized runtime {elapsed/(i+1)}')\n",
        "\n",
        "print(f'QKmeans retrained for {retrain} times')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yow0CRx80vJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.monotonic()\n",
        "kmeans = Kmeans(10)\n",
        "centers, assignments, loss = kmeans.run(data_4.copy())\n",
        "done = time.monotonic()\n",
        "elapsed = done - start\n",
        "print(f'Kmeans Clustering loss is {loss}, time is {elapsed}')\n",
        "\n",
        "epsilonList = [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25, 0.125]\n",
        "\n",
        "for e in epsilonList:\n",
        "  start = time.monotonic()\n",
        "  qKmeans = QKmeans(10, e)\n",
        "  centers, assignments, loss, retrain = qKmeans.run(data_4.copy())\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'QKmeans Clustering with epsilon {e} loss is {loss} using amortized runtime {elapsed}')\n",
        "\n",
        "widthList = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "\n",
        "for w in widthList:\n",
        "  ks = np.ones(2, dtype=int) * 10\n",
        "  widths = np.ones(2, dtype=int) * w\n",
        "  widths[0] = 1\n",
        "  start = time.monotonic()\n",
        "  dCKmeans = DCKmeans(ks, widths)\n",
        "  centers, assignments, loss = dCKmeans.run(data_4.copy(), assignments = True)\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'DCKmeans Clustering with width {w} loss is {loss} using amortized runtime {elapsed}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WerlyWv1MOD",
        "colab_type": "code",
        "outputId": "864b59a2-3585-4ea2-ea08-a7d7415bbfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn import datasets # for covtype\n",
        "cov = datasets.fetch_covtype().data #581012*54 => 2160*7=15120, done\n",
        "cov_y = datasets.fetch_covtype().target\n",
        "# print(cov.shape)\n",
        "# print(cov_y.shape)\n",
        "\n",
        "c1 = []\n",
        "c2 = []\n",
        "c3 = []\n",
        "c4 = []\n",
        "c5 = []\n",
        "c6 = []\n",
        "c7 = []\n",
        "for i in range(581012):  \n",
        "  if (cov_y[i]==1):\n",
        "    c1.append(cov[i])\n",
        "  if (cov_y[i]==2):\n",
        "    c2.append(cov[i])\n",
        "  if (cov_y[i]==3):\n",
        "    c3.append(cov[i])\n",
        "  if (cov_y[i]==4):\n",
        "    c4.append(cov[i])   \n",
        "  if (cov_y[i]==5):\n",
        "    c5.append(cov[i])  \n",
        "  if (cov_y[i]==6):\n",
        "    c6.append(cov[i])          \n",
        "  if (cov_y[i]==7):\n",
        "    c7.append(cov[i])\n",
        "\n",
        "print(len(c1)+len(c2)+len(c3)+len(c4)+len(c5)+len(c6)+len(c7))\n",
        "\n",
        "# shuffle to have 2160 samples from each category\n",
        "c1 = random.sample(c1, 2160)\n",
        "c2 = random.sample(c2, 2160)\n",
        "c3 = random.sample(c3, 2160)\n",
        "c4 = random.sample(c4, 2160)\n",
        "c5 = random.sample(c5, 2160)\n",
        "c6 = random.sample(c6, 2160)\n",
        "c7 = random.sample(c7, 2160)\n",
        "\n",
        "# print(len(c1))\n",
        "cov_selected = c1+c2+c3+c4+c5+c6+c7\n",
        "cov_selected = np.asarray(cov_selected)\n",
        "print(cov_selected.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976039\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "581012\n",
            "(15120, 54)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plcBMtoc6j_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = Kmeans(7)\n",
        "centers, assignments, loss = kmeans.run(cov_selected.copy())\n",
        "print(f'Kmeans Clustering loss is {loss}')\n",
        "\n",
        "qKmeans = QKmeans(7, 0.05)\n",
        "centers, assignments, loss, retrain = qKmeans.run(cov_selected.copy())\n",
        "print(f'QKmeans Clustering loss is {loss}')\n",
        "print(f'QKmeans Clustering retrain is {retrain}')\n",
        "\n",
        "numDelete = 1000\n",
        "\n",
        "start = time.monotonic()\n",
        "\n",
        "for i in range(numDelete):\n",
        "  centers, assignments, loss, retrain = qKmeans.delete(random.randint(0,qKmeans.data.shape[0]))\n",
        "  if ((i+1) % 40) == 0:\n",
        "    done = time.monotonic()\n",
        "    elapsed = done - start\n",
        "    print(elapsed)\n",
        "    print(f'QKmeans deleting {i+1} data retrained for {retrain} times using amortized runtime {elapsed/(i+1)}')\n",
        "\n",
        "print(f'QKmeans retrained for {retrain} times')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zorZeh5898Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.monotonic()\n",
        "kmeans = Kmeans(7)\n",
        "centers, assignments, loss = kmeans.run(cov_selected.copy())\n",
        "done = time.monotonic()\n",
        "elapsed = done - start\n",
        "print(f'Kmeans Clustering loss is {loss}, time is {elapsed}')\n",
        "\n",
        "epsilonList = [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25, 0.125]\n",
        "\n",
        "for e in epsilonList:\n",
        "  start = time.monotonic()\n",
        "  qKmeans = QKmeans(7, e)\n",
        "  centers, assignments, loss, retrain = qKmeans.run(cov_selected.copy())\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'QKmeans Clustering with epsilon {e} loss is {loss} using amortized runtime {elapsed}')\n",
        "\n",
        "widthList = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "\n",
        "for w in widthList:\n",
        "  ks = np.ones(2, dtype=int) * 7\n",
        "  widths = np.ones(2, dtype=int) * w\n",
        "  widths[0] = 1\n",
        "  start = time.monotonic()\n",
        "  dCKmeans = DCKmeans(ks, widths)\n",
        "  centers, assignments, loss = dCKmeans.run(cov_selected.copy(), assignments = True)\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'DCKmeans Clustering with width {w} loss is {loss} using amortized runtime {elapsed}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgYtB62A5JiM",
        "colab_type": "code",
        "outputId": "d31ff098-887b-48f7-c0da-d4a8bd154db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Postures\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':'15C6GzbiBkjC9qDIpJZxYkdviBDCmL0p-'})\n",
        "downloaded.GetContentFile('allUsers.lcl.csv') \n",
        "postures = pd.read_csv('allUsers.lcl.csv')\n",
        "postures = np.asarray(postures)\n",
        "postures = np.where(postures[1:]=='?', 300, postures[1:])\n",
        "print(postures.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78095, 38)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM_nNuuZ5wOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = Kmeans(5)\n",
        "centers, assignments, loss = kmeans.run(postures.copy())\n",
        "print(f'Kmeans Clustering loss is {loss}')\n",
        "\n",
        "qKmeans = QKmeans(5, 0.05)\n",
        "centers, assignments, loss, retrain = qKmeans.run(postures.copy())\n",
        "print(f'QKmeans Clustering loss is {loss}')\n",
        "print(f'QKmeans Clustering retrain is {retrain}')\n",
        "\n",
        "numDelete = 1000\n",
        "\n",
        "start = time.monotonic()\n",
        "\n",
        "for i in range(numDelete):\n",
        "  centers, assignments, loss, retrain = qKmeans.delete(random.randint(0,qKmeans.data.shape[0]))\n",
        "  if ((i+1) % 40) == 0:\n",
        "    done = time.monotonic()\n",
        "    elapsed = done - start\n",
        "    print(elapsed)\n",
        "    print(f'QKmeans deleting {i+1} data retrained for {retrain} times using amortized runtime {elapsed/(i+1)}')\n",
        "\n",
        "print(f'QKmeans retrained for {retrain} times')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nte5UH5562A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.monotonic()\n",
        "kmeans = Kmeans(5)\n",
        "centers, assignments, loss = kmeans.run(postures.copy())\n",
        "done = time.monotonic()\n",
        "elapsed = done - start\n",
        "print(f'Kmeans Clustering loss is {loss}, time is {elapsed}')\n",
        "\n",
        "epsilonList = [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25, 0.125]\n",
        "\n",
        "for e in epsilonList:\n",
        "  start = time.monotonic()\n",
        "  qKmeans = QKmeans(5, e)\n",
        "  centers, assignments, loss, retrain = qKmeans.run(postures.copy())\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'QKmeans Clustering with epsilon {e} loss is {loss} using amortized runtime {elapsed}')\n",
        "\n",
        "widthList = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "\n",
        "for w in widthList:\n",
        "  ks = np.ones(2, dtype=int) * 5\n",
        "  widths = np.ones(2, dtype=int) * w\n",
        "  widths[0] = 1\n",
        "  start = time.monotonic()\n",
        "  dCKmeans = DCKmeans(ks, widths)\n",
        "  centers, assignments, loss = dCKmeans.run(postures.copy(), assignments = True)\n",
        "  done = time.monotonic()\n",
        "  elapsed = done - start\n",
        "  print(f'DCKmeans Clustering with width {w} loss is {loss} using amortized runtime {elapsed}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}